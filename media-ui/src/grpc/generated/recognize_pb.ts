//
//This proto file has all the messages required for
//STT Service and its recognize gRPC calls

// @generated by protoc-gen-es v1.10.0 with parameter "target=ts"
// @generated from file recognize.proto (package com.cisco.wcc.ccai.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3, protoInt64 } from "@bufbuild/protobuf";

/**
 *
 * Return the Event based on the use input.
 * Similar events will be returned for Voice / DTMF based inputs.
 *
 * @generated from enum com.cisco.wcc.ccai.v1.OutputEvent
 */
export enum OutputEvent {
  /**
   * @generated from enum value: EVENT_UNSPECIFIED = 0;
   */
  EVENT_UNSPECIFIED = 0,

  /**
   *
   * Triggers when user utter the first utterance in Voice Input mode or First DTMF is pressed in DTMF Input mode.
   * This event to be used to BargeIn the prompt based on prompt barge-in flag.
   * The event will be sent only if the current prompt being played is bargein enabled or prompt playing is complete.
   *
   * @generated from enum value: EVENT_START_OF_INPUT = 1;
   */
  EVENT_START_OF_INPUT = 1,

  /**
   * Sent when user utterance Voice / DTMF is complete.
   *
   * @generated from enum value: EVENT_END_OF_INPUT = 2;
   */
  EVENT_END_OF_INPUT = 2,

  /**
   * Sent when utterance did not match any of the accepted input
   *
   * @generated from enum value: EVENT_NO_MATCH = 3;
   */
  EVENT_NO_MATCH = 3,

  /**
   * Sent when no audio received with in the expected timeframe
   *
   * @generated from enum value: EVENT_NO_INPUT = 4;
   */
  EVENT_NO_INPUT = 4,

  /**
   * Local VAD detects user utterance is complete
   *
   * @generated from enum value: VAD_END_OF_INPUT = 5;
   */
  VAD_END_OF_INPUT = 5,
}
// Retrieve enum metadata with: proto3.getEnumType(OutputEvent)
proto3.util.setEnumType(OutputEvent, "com.cisco.wcc.ccai.v1.OutputEvent", [
  { no: 0, name: "EVENT_UNSPECIFIED" },
  { no: 1, name: "EVENT_START_OF_INPUT" },
  { no: 2, name: "EVENT_END_OF_INPUT" },
  { no: 3, name: "EVENT_NO_MATCH" },
  { no: 4, name: "EVENT_NO_INPUT" },
  { no: 5, name: "VAD_END_OF_INPUT" },
]);

/**
 * BuildInGrammar enum
 *
 * @generated from enum com.cisco.wcc.ccai.v1.BuiltInGrammar
 */
export enum BuiltInGrammar {
  /**
   * @generated from enum value: Boolean = 0;
   */
  Boolean = 0,

  /**
   * @generated from enum value: Currency = 1;
   */
  Currency = 1,

  /**
   * @generated from enum value: Date = 2;
   */
  Date = 2,

  /**
   * @generated from enum value: Digits = 3;
   */
  Digits = 3,

  /**
   * @generated from enum value: Number = 4;
   */
  Number = 4,

  /**
   * @generated from enum value: Phone = 5;
   */
  Phone = 5,

  /**
   * @generated from enum value: Time = 6;
   */
  Time = 6,

  /**
   * @generated from enum value: Alphanum = 7;
   */
  Alphanum = 7,

  /**
   * @generated from enum value: Amount = 8;
   */
  Amount = 8,

  /**
   * @generated from enum value: Ordinal_number = 9;
   */
  Ordinal_number = 9,

  /**
   * @generated from enum value: Cardinal_number = 10;
   */
  Cardinal_number = 10,
}
// Retrieve enum metadata with: proto3.getEnumType(BuiltInGrammar)
proto3.util.setEnumType(BuiltInGrammar, "com.cisco.wcc.ccai.v1.BuiltInGrammar", [
  { no: 0, name: "Boolean" },
  { no: 1, name: "Currency" },
  { no: 2, name: "Date" },
  { no: 3, name: "Digits" },
  { no: 4, name: "Number" },
  { no: 5, name: "Phone" },
  { no: 6, name: "Time" },
  { no: 7, name: "Alphanum" },
  { no: 8, name: "Amount" },
  { no: 9, name: "Ordinal_number" },
  { no: 10, name: "Cardinal_number" },
]);

/**
 * Media type enum
 *
 * @generated from enum com.cisco.wcc.ccai.v1.EnumMediaType
 */
export enum EnumMediaType {
  /**
   * attempt to figure out the media type automatically.
   *
   * @generated from enum value: AUTOMATIC = 0;
   */
  AUTOMATIC = 0,

  /**
   * "application/srgs+xml"
   *
   * @generated from enum value: APPLICATION_SRGS_XML = 1;
   */
  APPLICATION_SRGS_XML = 1,

  /**
   * "application/x-swi-grammar"
   *
   * @generated from enum value: APPLICATION_X_SWI_GRAMMAR = 2;
   */
  APPLICATION_X_SWI_GRAMMAR = 2,

  /**
   * "application/x-swi-parameter"
   *
   * @generated from enum value: APPLICATION_X_SWI_PARAMETER = 3;
   */
  APPLICATION_X_SWI_PARAMETER = 3,
}
// Retrieve enum metadata with: proto3.getEnumType(EnumMediaType)
proto3.util.setEnumType(EnumMediaType, "com.cisco.wcc.ccai.v1.EnumMediaType", [
  { no: 0, name: "AUTOMATIC" },
  { no: 1, name: "APPLICATION_SRGS_XML" },
  { no: 2, name: "APPLICATION_X_SWI_GRAMMAR" },
  { no: 3, name: "APPLICATION_X_SWI_PARAMETER" },
]);

/**
 *
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 *
 * @generated from message com.cisco.wcc.ccai.v1.StreamingRecognitionResult
 */
export class StreamingRecognitionResult extends Message<StreamingRecognitionResult> {
  /**
   *
   * Output only. May contain one or more recognition hypotheses (up to the
   * maximum specified in `max_alternatives`).
   * These alternatives are ordered in terms of accuracy, with the top (first)
   * alternative being the most probable, as ranked by the recognizer.
   *
   * @generated from field: repeated com.cisco.wcc.ccai.v1.SpeechRecognitionAlternative alternatives = 1;
   */
  alternatives: SpeechRecognitionAlternative[] = [];

  /**
   * Output only. If `false`, this `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, this is the final time the
   * speech service will return this particular `StreamingRecognitionResult`,
   * the recognizer will not return any further hypotheses for this portion of
   * the transcript and corresponding audio.
   *
   * @generated from field: bool is_final = 2;
   */
  isFinal = false;

  /**
   *
   * Output only. Time offset of the end of this result relative to the
   * beginning of the audio.
   *
   * @generated from field: com.cisco.wcc.ccai.v1.Duration result_end_time = 4;
   */
  resultEndTime?: Duration;

  /**
   *
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For audio_channel_count = N, its output values can range from '1' to 'N'.
   *
   * @generated from field: int32 channel_tag = 5;
   */
  channelTag = 0;

  /**
   *
   * Output only. The
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
   * language in this result. This language code was detected to have the most
   * likelihood of being spoken in the audio.
   *
   * @generated from field: string language_code = 6;
   */
  languageCode = "";

  /**
   *
   * Whether or not recording offsets have been applied to the word
   * alignment values. Otherwise the word alignment start and end times
   * are only relative within the utterance.
   *
   * @generated from field: bool has_applied_recording_offsets = 7;
   */
  hasAppliedRecordingOffsets = false;

  /**
   *
   * Zero or more integers representing the speaker ID of this
   * result. This is usually derived from the speaker integers
   * that are passed in the streaming request.
   *
   * @generated from field: repeated uint32 speaker_ids = 8;
   */
  speakerIds: number[] = [];

  /**
   *
   * The unix time in milliseconds which was received from the client
   * for the StreamingRecognizeRequest that was last used to complete
   * this utterance.
   *
   * @generated from field: int64 last_packet_metrics_unix_timestamp_ms = 9;
   */
  lastPacketMetricsUnixTimestampMs = protoInt64.zero;

  /**
   * message type
   *
   * @generated from field: string message_type = 10;
   */
  messageType = "";

  /**
   * Event Based on user utterances.
   *
   * @generated from field: com.cisco.wcc.ccai.v1.OutputEvent response_event = 11;
   */
  responseEvent = OutputEvent.EVENT_UNSPECIFIED;

  /**
   * @generated from field: com.cisco.wcc.ccai.v1.StreamingRecognitionResult.Role role = 12;
   */
  role = StreamingRecognitionResult_Role.UNDEFINED;

  /**
   * indicate half close flag
   *
   * Indicates whether to stop the half-close operation
   *
   * @generated from field: bool stop_half_close = 13;
   */
  stopHalfClose = false;

  constructor(data?: PartialMessage<StreamingRecognitionResult>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.StreamingRecognitionResult";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "alternatives", kind: "message", T: SpeechRecognitionAlternative, repeated: true },
    { no: 2, name: "is_final", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 4, name: "result_end_time", kind: "message", T: Duration },
    { no: 5, name: "channel_tag", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 6, name: "language_code", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 7, name: "has_applied_recording_offsets", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 8, name: "speaker_ids", kind: "scalar", T: 13 /* ScalarType.UINT32 */, repeated: true },
    { no: 9, name: "last_packet_metrics_unix_timestamp_ms", kind: "scalar", T: 3 /* ScalarType.INT64 */ },
    { no: 10, name: "message_type", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 11, name: "response_event", kind: "enum", T: proto3.getEnumType(OutputEvent) },
    { no: 12, name: "role", kind: "enum", T: proto3.getEnumType(StreamingRecognitionResult_Role) },
    { no: 13, name: "stop_half_close", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): StreamingRecognitionResult {
    return new StreamingRecognitionResult().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): StreamingRecognitionResult {
    return new StreamingRecognitionResult().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): StreamingRecognitionResult {
    return new StreamingRecognitionResult().fromJsonString(jsonString, options);
  }

  static equals(a: StreamingRecognitionResult | PlainMessage<StreamingRecognitionResult> | undefined, b: StreamingRecognitionResult | PlainMessage<StreamingRecognitionResult> | undefined): boolean {
    return proto3.util.equals(StreamingRecognitionResult, a, b);
  }
}

/**
 * @generated from enum com.cisco.wcc.ccai.v1.StreamingRecognitionResult.Role
 */
export enum StreamingRecognitionResult_Role {
  /**
   * Role - Undefined
   *
   * @generated from enum value: UNDEFINED = 0;
   */
  UNDEFINED = 0,

  /**
   * Role - Caller
   *
   * @generated from enum value: CALLER = 1;
   */
  CALLER = 1,

  /**
   * Role - Agent
   *
   * @generated from enum value: AGENT = 2;
   */
  AGENT = 2,
}
// Retrieve enum metadata with: proto3.getEnumType(StreamingRecognitionResult_Role)
proto3.util.setEnumType(StreamingRecognitionResult_Role, "com.cisco.wcc.ccai.v1.StreamingRecognitionResult.Role", [
  { no: 0, name: "UNDEFINED" },
  { no: 1, name: "CALLER" },
  { no: 2, name: "AGENT" },
]);

/**
 *
 * Represents the Alternative hypotheses (a.k.a. n-best list).
 *
 * @generated from message com.cisco.wcc.ccai.v1.SpeechRecognitionAlternative
 */
export class SpeechRecognitionAlternative extends Message<SpeechRecognitionAlternative> {
  /**
   * Output only. Transcript text representing the words that the user spoke.
   *
   * @generated from field: string transcript = 1;
   */
  transcript = "";

  /**
   *
   * Output only. The confidence estimate between 0.0 and 1.0. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for the top alternative of a non-streaming
   * result or, of a streaming result where `is_final=true`.
   * Not yet supported.
   *
   * @generated from field: float confidence = 2;
   */
  confidence = 0;

  /**
   *
   * Output only. A list of word-specific information for each recognized word.
   * Note: When `enable_speaker_diarization` is true, you will see all the words
   * from the beginning of the audio.
   *
   * @generated from field: repeated com.cisco.wcc.ccai.v1.WordInfo words = 3;
   */
  words: WordInfo[] = [];

  constructor(data?: PartialMessage<SpeechRecognitionAlternative>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.SpeechRecognitionAlternative";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "transcript", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 3, name: "words", kind: "message", T: WordInfo, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SpeechRecognitionAlternative {
    return new SpeechRecognitionAlternative().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SpeechRecognitionAlternative {
    return new SpeechRecognitionAlternative().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SpeechRecognitionAlternative {
    return new SpeechRecognitionAlternative().fromJsonString(jsonString, options);
  }

  static equals(a: SpeechRecognitionAlternative | PlainMessage<SpeechRecognitionAlternative> | undefined, b: SpeechRecognitionAlternative | PlainMessage<SpeechRecognitionAlternative> | undefined): boolean {
    return proto3.util.equals(SpeechRecognitionAlternative, a, b);
  }
}

/**
 *
 * Represents the Word-specific information for recognized words.
 *
 * @generated from message com.cisco.wcc.ccai.v1.WordInfo
 */
export class WordInfo extends Message<WordInfo> {
  /**
   *
   * Output only. Time offset relative to the beginning of the audio,
   * and corresponding to the start of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   * This is an experimental feature and the accuracy of the time offset can
   * vary.
   *
   * @generated from field: com.cisco.wcc.ccai.v1.Duration start_time = 1;
   */
  startTime?: Duration;

  /**
   *
   * Output only. Time offset relative to the beginning of the audio,
   * and corresponding to the end of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   * This is an experimental feature and the accuracy of the time offset can
   * vary.
   *
   * @generated from field: com.cisco.wcc.ccai.v1.Duration end_time = 2;
   */
  endTime?: Duration;

  /**
   * Output only. The word corresponding to this set of information.
   *
   * @generated from field: string word = 3;
   */
  word = "";

  constructor(data?: PartialMessage<WordInfo>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.WordInfo";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "start_time", kind: "message", T: Duration },
    { no: 2, name: "end_time", kind: "message", T: Duration },
    { no: 3, name: "word", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): WordInfo {
    return new WordInfo().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): WordInfo {
    return new WordInfo().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): WordInfo {
    return new WordInfo().fromJsonString(jsonString, options);
  }

  static equals(a: WordInfo | PlainMessage<WordInfo> | undefined, b: WordInfo | PlainMessage<WordInfo> | undefined): boolean {
    return proto3.util.equals(WordInfo, a, b);
  }
}

/**
 *
 * Represents the Duration object denoting seconds and nanos
 *
 * @generated from message com.cisco.wcc.ccai.v1.Duration
 */
export class Duration extends Message<Duration> {
  /**
   *
   * Signed seconds of the span of time. Must be from -315,576,000,000
   * to +315,576,000,000 inclusive. Note: these bounds are computed from:
   * 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
   *
   * @generated from field: int64 seconds = 1;
   */
  seconds = protoInt64.zero;

  /**
   *
   * Signed fractions of a second at nanosecond resolution of the span
   * of time. Durations less than one second are represented with a 0
   * `seconds` field and a positive or negative `nanos` field. For durations
   * of one second or more, a non-zero value for the `nanos` field must be
   * of the same sign as the `seconds` field. Must be from -999,999,999
   * to +999,999,999 inclusive.
   *
   * @generated from field: int32 nanos = 2;
   */
  nanos = 0;

  constructor(data?: PartialMessage<Duration>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.Duration";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "seconds", kind: "scalar", T: 3 /* ScalarType.INT64 */ },
    { no: 2, name: "nanos", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Duration {
    return new Duration().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Duration {
    return new Duration().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Duration {
    return new Duration().fromJsonString(jsonString, options);
  }

  static equals(a: Duration | PlainMessage<Duration> | undefined, b: Duration | PlainMessage<Duration> | undefined): boolean {
    return proto3.util.equals(Duration, a, b);
  }
}

/**
 *
 * Represents the Status object with status code, message and description
 *
 * @generated from message com.cisco.wcc.ccai.v1.Status
 */
export class Status extends Message<Status> {
  /**
   *
   * The status code, which should be an enum value of
   * [google.rpc.Code][google.rpc.Code].
   *
   * @generated from field: int32 code = 1;
   */
  code = 0;

  /**
   *
   * A developer-facing error message, which should be in English. Any
   * user-facing error message should be localized and sent in the
   * [google.rpc.Status.details][google.rpc.Status.details] field, or localized
   * by the client.
   *
   * @generated from field: string message = 2;
   */
  message = "";

  /**
   * Longer description if available.
   *
   * @generated from field: string details = 3;
   */
  details = "";

  constructor(data?: PartialMessage<Status>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.Status";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "code", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 2, name: "message", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "details", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Status {
    return new Status().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Status {
    return new Status().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Status {
    return new Status().fromJsonString(jsonString, options);
  }

  static equals(a: Status | PlainMessage<Status> | undefined, b: Status | PlainMessage<Status> | undefined): boolean {
    return proto3.util.equals(Status, a, b);
  }
}

/**
 *
 * Represents the RecognitionFlags object with boolean to enable or disable timers
 *
 * @generated from message com.cisco.wcc.ccai.v1.RecognitionFlags
 */
export class RecognitionFlags extends Message<RecognitionFlags> {
  /**
   * Whether to disable recognition timers. By default, timers start when recognition begins.
   *
   * @generated from field: bool stall_timers = 1;
   */
  stallTimers = false;

  constructor(data?: PartialMessage<RecognitionFlags>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.RecognitionFlags";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "stall_timers", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): RecognitionFlags {
    return new RecognitionFlags().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): RecognitionFlags {
    return new RecognitionFlags().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): RecognitionFlags {
    return new RecognitionFlags().fromJsonString(jsonString, options);
  }

  static equals(a: RecognitionFlags | PlainMessage<RecognitionFlags> | undefined, b: RecognitionFlags | PlainMessage<RecognitionFlags> | undefined): boolean {
    return proto3.util.equals(RecognitionFlags, a, b);
  }
}

/**
 *
 * Represents the TimerInfo object with different stats
 *
 * @generated from message com.cisco.wcc.ccai.v1.SpeechTimers
 */
export class SpeechTimers extends Message<SpeechTimers> {
  /**
   * Maximum silence, in ms, allowed while waiting for user input after recognition timers are started. A value of 0 means no timeout.
   *
   * @generated from field: int32 no_input_timeout_ms = 1 [deprecated = true];
   * @deprecated
   */
  noInputTimeoutMs = 0;

  /**
   * Specify the duration of silence, in ms, after a valid recognition has occurred that determines the caller has finished speaking. Default is 0 (timer disabled).
   *
   * @generated from field: int32 complete_timeout_ms = 2;
   */
  completeTimeoutMs = 0;

  /**
   * Specify the duration of silence, in ms, after an utterance before concluding that the caller has finished speaking. Default is 1500 ms. A value of 0 disables the timer.
   *
   * @generated from field: int32 incomplete_timeout_ms = 3;
   */
  incompleteTimeoutMs = 0;

  /**
   * Maximum duration, in ms, of an utterance collected from the user. Default is 22000 ms (22 seconds). A value of -1 means no timeout.
   *
   * @generated from field: int32 max_speech_timeout_ms = 4;
   */
  maxSpeechTimeoutMs = 0;

  constructor(data?: PartialMessage<SpeechTimers>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.SpeechTimers";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "no_input_timeout_ms", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 2, name: "complete_timeout_ms", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "incomplete_timeout_ms", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 4, name: "max_speech_timeout_ms", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SpeechTimers {
    return new SpeechTimers().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SpeechTimers {
    return new SpeechTimers().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SpeechTimers {
    return new SpeechTimers().fromJsonString(jsonString, options);
  }

  static equals(a: SpeechTimers | PlainMessage<SpeechTimers> | undefined, b: SpeechTimers | PlainMessage<SpeechTimers> | undefined): boolean {
    return proto3.util.equals(SpeechTimers, a, b);
  }
}

/**
 *
 * DTMF Input Configurations
 *
 * @generated from message com.cisco.wcc.ccai.v1.DTMFConfig
 */
export class DTMFConfig extends Message<DTMFConfig> {
  /**
   * Timeout between two digits
   *
   * @generated from field: int32 inter_digit_timeout = 1;
   */
  interDigitTimeout = 0;

  /**
   * Char to terminate the DTMF inputs
   *
   * @generated from field: string termchar = 2;
   */
  termchar = "";

  /**
   * Max Length of DTMF to be collected
   *
   * @generated from field: int32 dtmf_length = 3;
   */
  dtmfLength = 0;

  constructor(data?: PartialMessage<DTMFConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.DTMFConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "inter_digit_timeout", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 2, name: "termchar", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "dtmf_length", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): DTMFConfig {
    return new DTMFConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): DTMFConfig {
    return new DTMFConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): DTMFConfig {
    return new DTMFConfig().fromJsonString(jsonString, options);
  }

  static equals(a: DTMFConfig | PlainMessage<DTMFConfig> | undefined, b: DTMFConfig | PlainMessage<DTMFConfig> | undefined): boolean {
    return proto3.util.equals(DTMFConfig, a, b);
  }
}

/**
 *
 * Represents the RecognitionResource object with attributes like grammar, language and weight
 *
 * @generated from message com.cisco.wcc.ccai.v1.RecognitionResource
 */
export class RecognitionResource extends Message<RecognitionResource> {
  /**
   * @generated from oneof com.cisco.wcc.ccai.v1.RecognitionResource.grammar
   */
  grammar: {
    /**
     * Name of a builtin resource supported by the installed language pack.
     *
     * @generated from field: com.cisco.wcc.ccai.v1.BuiltInGrammar builtInGrammar = 1;
     */
    value: BuiltInGrammar;
    case: "builtInGrammar";
  } | {
    /**
     * @generated from field: com.cisco.wcc.ccai.v1.InlineGrammar inlineGrammar = 5;
     */
    value: InlineGrammar;
    case: "inlineGrammar";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * Mandatory. Language and country (locale) code as xx-XX (2-letters format), e.g. en-US.
   *
   * @generated from field: string language = 2;
   */
  language = "";

  /**
   * Specifies the grammar's weight relative to other grammars active for that recognition. This value can range from 1 to 32767. Default is 1.
   *
   * @generated from field: int32 weight = 3;
   */
  weight = 0;

  /**
   * Specifies the id that Nuance Recognizer will use to identify the grammar in the recognition result. If not set, Nuance Recognizer will generate a unique one.
   *
   * @generated from field: string grammar_id = 4;
   */
  grammarId = "";

  constructor(data?: PartialMessage<RecognitionResource>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.RecognitionResource";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "builtInGrammar", kind: "enum", T: proto3.getEnumType(BuiltInGrammar), oneof: "grammar" },
    { no: 5, name: "inlineGrammar", kind: "message", T: InlineGrammar, oneof: "grammar" },
    { no: 2, name: "language", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "weight", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 4, name: "grammar_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): RecognitionResource {
    return new RecognitionResource().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): RecognitionResource {
    return new RecognitionResource().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): RecognitionResource {
    return new RecognitionResource().fromJsonString(jsonString, options);
  }

  static equals(a: RecognitionResource | PlainMessage<RecognitionResource> | undefined, b: RecognitionResource | PlainMessage<RecognitionResource> | undefined): boolean {
    return proto3.util.equals(RecognitionResource, a, b);
  }
}

/**
 *
 * InlineGrammar object with media type and grammar information
 *
 * @generated from message com.cisco.wcc.ccai.v1.InlineGrammar
 */
export class InlineGrammar extends Message<InlineGrammar> {
  /**
   * The type of media used for the inline grammar data. If not specified, let Nuance Recognizer detect the media type.
   *
   * @generated from field: com.cisco.wcc.ccai.v1.EnumMediaType media_type = 1;
   */
  mediaType = EnumMediaType.AUTOMATIC;

  /**
   * Grammar data
   *
   * @generated from field: string grammar = 2;
   */
  grammar = "";

  constructor(data?: PartialMessage<InlineGrammar>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.InlineGrammar";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "media_type", kind: "enum", T: proto3.getEnumType(EnumMediaType) },
    { no: 2, name: "grammar", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): InlineGrammar {
    return new InlineGrammar().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): InlineGrammar {
    return new InlineGrammar().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): InlineGrammar {
    return new InlineGrammar().fromJsonString(jsonString, options);
  }

  static equals(a: InlineGrammar | PlainMessage<InlineGrammar> | undefined, b: InlineGrammar | PlainMessage<InlineGrammar> | undefined): boolean {
    return proto3.util.equals(InlineGrammar, a, b);
  }
}

/**
 * @generated from message com.cisco.wcc.ccai.v1.Model
 */
export class Model extends Message<Model> {
  /**
   * Name of the model to be used for recognition
   *
   * @generated from field: string model_name = 1;
   */
  modelName = "";

  /**
   * Variant of the model to be used for recognition
   *
   * @generated from field: string model_variant = 2;
   */
  modelVariant = "";

  constructor(data?: PartialMessage<Model>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.Model";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "model_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "model_variant", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Model {
    return new Model().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Model {
    return new Model().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Model {
    return new Model().fromJsonString(jsonString, options);
  }

  static equals(a: Model | PlainMessage<Model> | undefined, b: Model | PlainMessage<Model> | undefined): boolean {
    return proto3.util.equals(Model, a, b);
  }
}

/**
 *
 * Represents a recognition request config
 *
 * @generated from message com.cisco.wcc.ccai.v1.RecognitionRequestConfig
 */
export class RecognitionRequestConfig extends Message<RecognitionRequestConfig> {
  /**
   *
   * Encoding of audio data sent in all `RecognitionAudio` messages.
   * This field is optional for `FLAC` and `WAV` audio files and required
   * for all other audio formats. For details, see
   * [AudioEncoding][google.cloud.speech.v1.RecognitionConfig.AudioEncoding].
   *
   * @generated from field: com.cisco.wcc.ccai.v1.RecognitionRequestConfig.AudioEncoding encoding = 1;
   */
  encoding = RecognitionRequestConfig_AudioEncoding.ENCODING_UNSPECIFIED;

  /**
   *
   * Sample rate in Hertz of the audio data sent in all
   * `RecognitionAudio` messages.
   *
   * @generated from field: int32 sample_rate_hertz = 2;
   */
  sampleRateHertz = 0;

  /**
   * Currently, only en-US is supported.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode = "";

  /**
   *
   * Optional. Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognitionAlternative` messages
   * within each `SpeechRecognitionResult`.
   * The server may return fewer than `max_alternatives`.
   * Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
   * one. If omitted, will return a maximum of one.
   *
   * @generated from field: int32 max_alternatives = 4;
   */
  maxAlternatives = 0;

  /**
   *
   * Optional. If set to `true`, the server will attempt to filter out
   * profanities, replacing all but the initial character in each filtered word
   * with asterisks, e.g. "f***". If set to `false` or omitted, profanities
   * won't be filtered out.
   * Not yet supported.
   *
   * @generated from field: bool profanity_filter = 5;
   */
  profanityFilter = false;

  /**
   *
   * Optional. array of [SpeechContext][google.cloud.speech.v1.SpeechContext].
   * A means to provide context to assist the speech recognition. For more
   * information, see [Phrase Hints](/speech-to-text/docs/basics#phrase-hints).
   *
   * @generated from field: repeated com.cisco.wcc.ccai.v1.SpeechContext speech_contexts = 6;
   */
  speechContexts: SpeechContext[] = [];

  /**
   *
   * Optional. If `false` or omitted, the recognizer will perform continuous
   * recognition (continuing to wait for and process audio even if the user
   * pauses speaking) until the client closes the input stream (gRPC API) or
   * until the maximum time limit has been reached. May return multiple
   * `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
   *
   * If `true`, the recognizer will detect a single spoken utterance. When it
   * detects that the user has paused or stopped speaking, it will return an
   * `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
   * more than one `StreamingRecognitionResult` with the `is_final` flag set to
   * `true`.
   *
   * @generated from field: bool single_utterance = 7;
   */
  singleUtterance = false;

  /**
   *
   * Optional If `true`, interim results (tentative hypotheses) may be
   * returned as they become available (these interim results are indicated with
   * the `is_final=false` flag).
   * If `false` or omitted, only `is_final=true` result(s) are returned.
   *
   * @generated from field: bool interim_results = 8;
   */
  interimResults = false;

  /**
   *
   * Optional. If 'true', adds punctuation to recognition result hypotheses.
   * Not yet supported.
   *
   * @generated from field: bool enable_automatic_punctuation = 9;
   */
  enableAutomaticPunctuation = false;

  /**
   * Optional. Pass timer info for the recognizer
   *
   * @generated from field: com.cisco.wcc.ccai.v1.SpeechTimers speech_timers = 10;
   */
  speechTimers?: SpeechTimers;

  /**
   * A balance between detecting speech and noise (breathing, etc.), 0 to 1.0. 0 means ignore all noise, 1.0 means interpret all noise as speech. Default is 0.5.
   *
   * @generated from field: float speech_detection_sensitivity = 11;
   */
  speechDetectionSensitivity = 0;

  /**
   * Maximum number of n-best hypotheses to return. Range is 0 to 999. Additional CPU cycles needed if > 5. Default is 2.
   *
   * @generated from field: int32 nbest = 12;
   */
  nbest = 0;

  /**
   * When the score of the first n-best entry is less than the value of confidencelevel, the recognition will return a no-match. Range is 0 to 1.0. Default is 0 (all utterances accepted).
   *
   * @generated from field: float confidence_level = 13;
   */
  confidenceLevel = 0;

  /**
   * Channel number which has the caller stream in a multichannel file URL. Other streams will be considered as Agent Stream.
   * Default value will be based on RTMS usecase
   *
   * @generated from field: int32 caller_channel_number = 14;
   */
  callerChannelNumber = 0;

  /**
   * Model to be used for recognition
   *
   * @generated from field: com.cisco.wcc.ccai.v1.Model model = 15;
   */
  model?: Model;

  constructor(data?: PartialMessage<RecognitionRequestConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.RecognitionRequestConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "encoding", kind: "enum", T: proto3.getEnumType(RecognitionRequestConfig_AudioEncoding) },
    { no: 2, name: "sample_rate_hertz", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "language_code", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "max_alternatives", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 5, name: "profanity_filter", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 6, name: "speech_contexts", kind: "message", T: SpeechContext, repeated: true },
    { no: 7, name: "single_utterance", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 8, name: "interim_results", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 9, name: "enable_automatic_punctuation", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 10, name: "speech_timers", kind: "message", T: SpeechTimers },
    { no: 11, name: "speech_detection_sensitivity", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 12, name: "nbest", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 13, name: "confidence_level", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 14, name: "caller_channel_number", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 15, name: "model", kind: "message", T: Model },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): RecognitionRequestConfig {
    return new RecognitionRequestConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): RecognitionRequestConfig {
    return new RecognitionRequestConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): RecognitionRequestConfig {
    return new RecognitionRequestConfig().fromJsonString(jsonString, options);
  }

  static equals(a: RecognitionRequestConfig | PlainMessage<RecognitionRequestConfig> | undefined, b: RecognitionRequestConfig | PlainMessage<RecognitionRequestConfig> | undefined): boolean {
    return proto3.util.equals(RecognitionRequestConfig, a, b);
  }
}

/**
 *
 * The encoding of the audio data sent in the request.
 *
 * All encodings support only 1 channel (mono) audio.
 *
 * @generated from enum com.cisco.wcc.ccai.v1.RecognitionRequestConfig.AudioEncoding
 */
export enum RecognitionRequestConfig_AudioEncoding {
  /**
   * Not specified.
   *
   * @generated from enum value: ENCODING_UNSPECIFIED = 0;
   */
  ENCODING_UNSPECIFIED = 0,

  /**
   * Uncompressed 16-bit signed little-endian samples (Linear PCM).
   *
   * @generated from enum value: LINEAR16 = 1;
   */
  LINEAR16 = 1,

  /**
   * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
   *
   * @generated from enum value: MULAW = 3;
   */
  MULAW = 3,

  /**
   * 8-bit samples that compand 14-bit audio samples using G.711 PCMU/a-law.
   *
   * @generated from enum value: ALAW = 4;
   */
  ALAW = 4,
}
// Retrieve enum metadata with: proto3.getEnumType(RecognitionRequestConfig_AudioEncoding)
proto3.util.setEnumType(RecognitionRequestConfig_AudioEncoding, "com.cisco.wcc.ccai.v1.RecognitionRequestConfig.AudioEncoding", [
  { no: 0, name: "ENCODING_UNSPECIFIED" },
  { no: 1, name: "LINEAR16" },
  { no: 3, name: "MULAW" },
  { no: 4, name: "ALAW" },
]);

/**
 *
 * Represents a speech context message
 *
 * @generated from message com.cisco.wcc.ccai.v1.SpeechContext
 */
export class SpeechContext extends Message<SpeechContext> {
  /**
   *
   * Optional. A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer.
   *
   * @generated from field: repeated string phrases = 1;
   */
  phrases: string[] = [];

  constructor(data?: PartialMessage<SpeechContext>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "com.cisco.wcc.ccai.v1.SpeechContext";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "phrases", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SpeechContext {
    return new SpeechContext().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SpeechContext {
    return new SpeechContext().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SpeechContext {
    return new SpeechContext().fromJsonString(jsonString, options);
  }

  static equals(a: SpeechContext | PlainMessage<SpeechContext> | undefined, b: SpeechContext | PlainMessage<SpeechContext> | undefined): boolean {
    return proto3.util.equals(SpeechContext, a, b);
  }
}

